{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFyb0y7PUJOo"
   },
   "source": [
    "# 1D-Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9vTr2NhcGAA"
   },
   "source": [
    "# Test GPU\n",
    "Before Starting, kindly check the available GPU from the Google Server, GPU model and other related information. It might help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"Is CUDA enabled GPU Available?\", torch.cuda.is_available())\n",
    "print(\"GPU Number:\", torch.cuda.device_count())\n",
    "print(\"Current GPU Index:\", torch.cuda.current_device())\n",
    "print(\"GPU Type:\", torch.cuda.get_device_name(device=None))\n",
    "print(\"GPU Capability:\", torch.cuda.get_device_capability(device=None))\n",
    "print(\"Is GPU Initialized yet?\", torch.cuda.is_initialized())\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.config.experimental.list_physical_devices())\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgW7r0C9TuZk"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eMhBhz1CrMb3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import scipy\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.io as sio\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, classification_report, confusion_matrix, root_mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZAo2XBvzFX49"
   },
   "outputs": [],
   "source": [
    "# from segmentation_models import *\n",
    "from segmentation_models.unet_variants import *\n",
    "# from segmentation_models.MLMRSNet import *\n",
    "# from segmentation_models.saunet_variants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "03JA1kRfzoit"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4tWmjn_kMZu"
   },
   "source": [
    "# Set Number of Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YZtdPlqmQ7Tj"
   },
   "outputs": [],
   "source": [
    "num_channel = 3\n",
    "fold = 1\n",
    "device = 'Kettle'\n",
    "phase = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overlap Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleanse Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Train, Test and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "Checkpoint = {}\n",
    "Checkpoint['X'] = X_Train\n",
    "Checkpoint['Y'] = Y_Train\n",
    "RawDataPath = 'Train_Set.pt'\n",
    "torch.save(Checkpoint, RawDataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "Checkpoint = {}\n",
    "Checkpoint['X'] = X_Val\n",
    "Checkpoint['Y'] = Y_Val\n",
    "RawDataPath = 'Val_Set.pt'\n",
    "torch.save(Checkpoint, RawDataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "Checkpoint = {}\n",
    "Checkpoint['X'] = X_Test\n",
    "Checkpoint['Y'] = Y_Test\n",
    "RawDataPath = 'Test_Set.pt'\n",
    "torch.save(Checkpoint, RawDataPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qTwozk_BS94"
   },
   "source": [
    "### Garbage Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1633010341331,
     "user": {
      "displayName": "Sakib Mahmud",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg8lG2uTygQr7y6fmQUo67XXUtrCVGaEakj_P33Ft8=s64",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "ch2jmn3jKKiH",
    "outputId": "c9807e23-ff6c-428d-f6e3-4f5e6382e71d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc #Garbage Collector\n",
    "fl_Data = None\n",
    "X_Test = None\n",
    "X_Train = None\n",
    "X_Val = None\n",
    "Y_Test = None\n",
    "Y_Train = None\n",
    "Y_Val = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHq0FrX9iAsq"
   },
   "source": [
    "# MAIN (Sample Application)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yozhkF-OJWu2"
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Mai6ZRMeiFvA"
   },
   "outputs": [],
   "source": [
    "# Configurations\n",
    "## General Configurations\n",
    "signal_length = 21600  # Length of each Segment\n",
    "model_name = 'LDNet'  # UNet, UNetPP, etc.\n",
    "model_depth = 4  # Number of Level in the CNN Model\n",
    "model_width = 64  # Width of the Initial Layer, subsequent layers start from here\n",
    "kernel_size = 3  # Size of the Kernels/Filter\n",
    "num_channel = 3 # Number of Channels in the Model\n",
    "D_S = 1  # Turn on Deep Supervision\n",
    "A_E = 0  # Turn on AutoEncoder Mode for Feature Extraction\n",
    "A_G = 0  # Turn on for Guided Attention\n",
    "LSTM = 0  # Turn on BiConvLSTM Block\n",
    "problem_type = 'Regression'\n",
    "output_nums = 1  # Number of Class for Classification Problems, always '1' for Regression Problems\n",
    "is_transconv = True # True: Transposed Convolution, False: UpSampling\n",
    "feature_number = 1024  # Number of Features to be Extracted, only required if the AutoEncoder Mode is turned on\n",
    "\n",
    "## Model Specific Configurations\n",
    "alpha = 1  # Model Width Expansion Parameter, for MultiResUNet only\n",
    "pooling_type = 'mix' # pooling_type, only for EMARS\n",
    "cardinality = 5  # Cardinality, only for EMARS\n",
    "q = 3\n",
    "t = 1\n",
    "\n",
    "## Data Configurations\n",
    "DS_Model_Type = 'UNetPP' # UNet or UNetPP\n",
    "\n",
    "## Experimental Configurations8\n",
    "load_weights = False\n",
    "max_epoch_stop = 20\n",
    "max_epoch_lr_change = 10\n",
    "lr = 3e-4\n",
    "lr_factor = 0.1\n",
    "monitor_param = 'loss'  # 'loss' or 'acc' for monitoring\n",
    "\n",
    "# Loss Weight Dictionary for Deep Supervision\n",
    "if D_S == 1:\n",
    "    loss_weights = np.zeros(model_depth)\n",
    "    for k in range(0, model_depth):\n",
    "        loss_weights[k] = 1-(k*0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGoiZp6_JpIZ"
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Jms3ad41Joam"
   },
   "outputs": [],
   "source": [
    "def prepareTrainDict(y, model_depth, signal_length, model_name, num_channel=1):\n",
    "    def approximate(inp, w_len, signal_length, num_channel):\n",
    "        ops = np.zeros((len(inp), signal_length//w_len, num_channel))\n",
    "        for j in range(0,num_channel):\n",
    "            op = np.zeros((len(inp), signal_length//w_len))\n",
    "            for i in range(0, signal_length, w_len):\n",
    "                try:\n",
    "                    op[:, i//w_len] = np.mean(inp[:, i:i+w_len, j], axis=1)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print(i)\n",
    "            ops[:,:,j] = op\n",
    "        return ops\n",
    "\n",
    "    out = {}\n",
    "    Y_Train_dict = {}\n",
    "    out['out'] = np.array(y)\n",
    "    Y_Train_dict['out'] = out['out']\n",
    "    for i in range(1, (model_depth+1)):\n",
    "        name = f'level{i}'\n",
    "        if model_name == 'UNet':\n",
    "            out[name] = approximate(y, 2**i, signal_length, num_channel)\n",
    "        elif model_name == 'UNetPP':\n",
    "            out[name] = y\n",
    "        Y_Train_dict[f'level{i}'] = out[f'level{i}']\n",
    "\n",
    "    return Y_Train_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWQQ-U2bJ2Js"
   },
   "source": [
    "## Train and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnvML-ORy1uq"
   },
   "source": [
    "Build Model for 1D Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model for PPG2ABP Segmentation - Deep UNet Architecture\n",
    "SMDisAgg_Network = MLMRSNet(signal_length, model_depth, num_channel, model_width, kernel_size, problem_type='Regression', output_nums=output_nums,\n",
    "                            ds=D_S, ae=A_E, cardinality=cardinality, pooling_type=pooling_type).LDNet()\n",
    "# print(SMDisAgg_Network.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "Train_Data = torch.load('Train_Set.pt')\n",
    "Test_Data = torch.load('Test_Set.pt')\n",
    "Val_Data = torch.load('Train_Set.pt')\n",
    "X_Train = Train_Data['X']\n",
    "Y_Train = Train_Data['Y']\n",
    "X_Test = Test_Data['X']\n",
    "Y_Test = Test_Data['Y']\n",
    "X_Val = Val_Data['X']\n",
    "Y_Val = Val_Data['Y']\n",
    "if D_S == 0:  # Deep Supervision OFF\n",
    "    # Compile Built Model\n",
    "    SMDisAgg_Network.compile(loss=tf.keras.losses.MeanAbsoluteError(), optimizer=tf.keras.optimizers.Adam(learning_rate=lr), metrics=tf.keras.metrics.MeanSquaredError())\n",
    "    # Directory for Saving Trained Models\n",
    "    save_directory = f'trained_models/{model_name}/'+model_name+'_'+str(signal_length)+'_'+str(model_width)+'_'+str(num_channel)+'_'+str(D_S)+'.h5'\n",
    "    # Load Pretrained Weights (if available)\n",
    "    if (os.path.exists(save_directory) and load_weights == True):\n",
    "        print('\\nLoading Pretrained Weights...')\n",
    "        # Load Previously Trained Weights for Transfer Learning\n",
    "        SMDisAgg_Network.load_weights(save_directory)\n",
    "    # Declare Callbacks\n",
    "    if monitor_param == 'loss':\n",
    "        callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=max_epoch_stop, mode='min'),\n",
    "                    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=lr_factor, patience=max_epoch_lr_change, verbose=1, mode='min'),\n",
    "                    tf.keras.callbacks.ModelCheckpoint(save_directory, verbose=1, monitor='val_loss', save_best_only=True, mode='min')]\n",
    "    elif monitor_param == 'acc':\n",
    "        callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=max_epoch_stop, mode='max'),\n",
    "                    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=lr_factor, patience=max_epoch_lr_change, verbose=1, mode='max'),\n",
    "                    tf.keras.callbacks.ModelCheckpoint(save_directory, verbose=1, monitor='val_acc', save_best_only=True, mode='max')]\n",
    "    # Train Model\n",
    "    print('Starting Training...')\n",
    "    model_history = SMDisAgg_Network.fit(X_Train, Y_Train, epochs=500, batch_size=1, verbose=1, validation_data=(X_Val, Y_Val), shuffle=True, callbacks=callbacks)\n",
    "    # Test and Evaluation\n",
    "    Y_Pred = SMDisAgg_Network.predict(X_Test, verbose=0)\n",
    "    # Save History\n",
    "    print('\\n')\n",
    "    print('Save History')\n",
    "    # Get the dictionary containing each metric and the loss for each epoch\n",
    "    history_dict = model_history.history\n",
    "    history_path = f'Outcomes/{model_name}_{device}_History.h5'\n",
    "    json.dump(history_dict.item(), open(history_path, 'w'))\n",
    "    print('\\n')\n",
    "    # Save Outcomes\n",
    "    print('Save Results')\n",
    "    File = h5py.File(f'Outcomes/{model_name}_Ch{num_channel}_F{fold}_P{phase}_D{device}.h5', 'w')\n",
    "    File.create_dataset('SM', data=X_Test)\n",
    "    File.create_dataset('App', data=Y_Test)\n",
    "    File.create_dataset('App_Pred', data=Y_Pred)\n",
    "    File.close()\n",
    "    print('\\n')\n",
    "elif D_S == 1:  # Deep Supervision ON\n",
    "    # Prepare Train and Test Sets for Deep Supervision\n",
    "    Y_Train_dict = prepareTrainDict(Y_Train, model_depth, signal_length, DS_Model_Type)\n",
    "    Y_Val_dict = prepareTrainDict(Y_Val, model_depth, signal_length, DS_Model_Type)\n",
    "    # Generate Custom Loss Weights for Deep Supervision\n",
    "    loss_weights = np.zeros(model_depth)\n",
    "    for k in range(0, model_depth):\n",
    "        loss_weights[k] = 1-(k*0.1)\n",
    "    # Compile Built Model\n",
    "    SMDisAgg_Network.compile(loss=tf.keras.losses.MeanAbsoluteError(), optimizer=tf.keras.optimizers.Adam(learning_rate=lr), metrics=tf.keras.metrics.MeanSquaredError(), loss_weights=loss_weights)\n",
    "    # Directory for Saving Trained Models\n",
    "    save_directory = f'trained_models/{model_name}/'+model_name+'_'+str(signal_length)+'_'+str(model_width)+'_'+str(num_channel)+'_'+str(D_S)+'.h5'\n",
    "    # Load Pretrained Weights (if available)\n",
    "    if (os.path.exists(save_directory) and load_weights == True):\n",
    "        print('Loading Pretrained Weights...')\n",
    "        # Load Previously Trained Weights for Transfer Learning\n",
    "        SMDisAgg_Network.load_weights(save_directory)\n",
    "    # Declare Callbacks\n",
    "    if monitor_param == 'loss':\n",
    "        callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_out_loss', patience=max_epoch_stop, mode='min'),\n",
    "                    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_out_loss', factor=lr_factor, patience=max_epoch_lr_change, verbose=1, mode='min'),\n",
    "                    tf.keras.callbacks.ModelCheckpoint(save_directory, verbose=1, monitor='val_out_loss', save_best_only=True, mode='min')]\n",
    "    elif monitor_param == 'acc':\n",
    "        callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_out_acc', patience=max_epoch_stop, mode='max'),\n",
    "                    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_out_acc', factor=lr_factor, patience=max_epoch_lr_change, verbose=1, mode='max'),\n",
    "                    tf.keras.callbacks.ModelCheckpoint(save_directory, verbose=1, monitor='val_out_acc', save_best_only=True, mode='max')]\n",
    "    # Train Model\n",
    "    print('Starting Training...')\n",
    "    model_history = SMDisAgg_Network.fit(X_Train, Y_Train_dict, epochs=500, batch_size=2, verbose=1, validation_data=(X_Val, Y_Val_dict), shuffle=True, callbacks=callbacks)\n",
    "    # Test and Evaluation\n",
    "    Y_Pred = SMDisAgg_Network.predict(X_Test, verbose=0)\n",
    "    Y_Pred = Y_Pred[0]\n",
    "    '''Test and Evaluation'''\n",
    "    # Save History\n",
    "    print('\\n')\n",
    "    print('Save History')\n",
    "    # Get the dictionary containing each metric and the loss for each epoch\n",
    "    history_dict = model_history.history\n",
    "    history_path = f'History/{model_name}_{device}_History.h5'\n",
    "    with open(history_path, 'wb') as file:\n",
    "        history_dict = model_history.history\n",
    "        pickle.dump(history_dict, file, pickle.HIGHEST_PROTOCOL)\n",
    "    print('\\n')\n",
    "    # Save Outcomes\n",
    "    print('Save Results')\n",
    "    File = h5py.File(f'Outcomes/{model_name}_Ch{num_channel}_F{fold}_P{phase}_{device}.h5', 'w')\n",
    "    File.create_dataset('SM', data=X_Test)\n",
    "    File.create_dataset('App', data=Y_Test)\n",
    "    File.create_dataset('App_Pred', data=Y_Pred)\n",
    "    File.close()\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "21"
    }
   },
   "outputs": [],
   "source": [
    "# Save History\n",
    "print('\\n')\n",
    "print('Save History')\n",
    "# Get the dictionary containing each metric and the loss for each epoch\n",
    "history_dict = model_history.history\n",
    "history_path = f'History/{model_name}_{device}_History.h5'\n",
    "with open(history_path, 'wb') as file:\n",
    "    history_dict = model_history.history\n",
    "    pickle.dump(history_dict, file, pickle.HIGHEST_PROTOCOL)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(history_path, 'rb') as file:\n",
    "    history=pickle.load(file)\n",
    "print(history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgJMiRUPy7aC"
   },
   "source": [
    "External Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tbs1Wo3vw6c0"
   },
   "outputs": [],
   "source": [
    "SMDisAgg_Network = MLMRSNet(signal_length, model_depth, num_channel, model_width, kernel_size, problem_type='Regression', output_nums=output_nums,\n",
    "                            ds=D_S, ae=A_E, cardinality=cardinality, pooling_type=pooling_type).LDNet()\n",
    "if D_S == 0:\n",
    "    SMDisAgg_Network.compile(loss=tf.keras.losses.MeanAbsoluteError(), optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), metrics=tf.keras.metrics.MeanSquaredError())\n",
    "elif D_S == 1:\n",
    "    SMDisAgg_Network.compile(loss=tf.keras.losses.MeanAbsoluteError(), optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), metrics=tf.keras.metrics.MeanSquaredError(), loss_weights= loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76, 21600, 3)\n",
      "(76, 21600, 1)\n",
      "(76,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-30 18:05:13.704342: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-05-30 18:05:14.038925: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-30 18:05:14.282044: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76, 21600, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load Model and Predict\n",
    "save_directory = f'trained_models/{model_name}/'+model_name+'_'+str(signal_length)+'_'+str(model_width)+'_'+str(num_channel)+'_'+str(D_S)+'.h5'\n",
    "SMDisAgg_Network.load_weights(save_directory)\n",
    "# Load Test Set\n",
    "Test_Data = torch.load('Test_Set.pt')\n",
    "X_Test = Test_Data['X']\n",
    "Y_Test = Test_Data['Y']\n",
    "house_labels = Test_Data['Z']\n",
    "print(X_Test.shape)\n",
    "print(Y_Test.shape)\n",
    "print(house_labels.shape)\n",
    "# Predict\n",
    "Y_Pred = SMDisAgg_Network.predict(X_Test, verbose=0)\n",
    "Y_Pred = Y_Pred[0]\n",
    "print(Y_Pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Results\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save Outcomes\n",
    "print('Save Results')\n",
    "File = h5py.File(f'Outcomes/{model_name}_Ch{num_channel}_F{fold}_P{phase}_{device}.h5', 'w')\n",
    "File.create_dataset('SM', data=X_Test)\n",
    "File.create_dataset('App', data=Y_Test)\n",
    "File.create_dataset('App_Pred', data=Y_Pred)\n",
    "File.close()\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['App', 'App_Pred', 'SM']>\n"
     ]
    }
   ],
   "source": [
    "# Load Resuts with Ground Truth and Inputs\n",
    "fl_Data = h5py.File(os.path.join('Outcomes/LDNet_Ch3_F1_P1_Kettle.h5'),'r')\n",
    "print(fl_Data.keys())\n",
    "X_Test = fl_Data['SM']\n",
    "Y_Test = fl_Data['App']\n",
    "Y_Pred = fl_Data['App_Pred']\n",
    "# Y_Test_Denorm = fl_Data['App_Denorm']\n",
    "# Y_Pred_Denorm = fl_Data['App_Pred_Denorm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68, 21600, 3)\n",
      "(68, 21600, 1)\n",
      "(68,)\n",
      "(12,)\n",
      "<KeysViewHDF5 ['App', 'App_Pred', 'SM']>\n"
     ]
    }
   ],
   "source": [
    "# Load Test Set and Print Shape\n",
    "Test_Data = torch.load('Test_Set.pt')\n",
    "X_Test = Test_Data['X']\n",
    "Y_Test = Test_Data['Y']\n",
    "house_labels = np.int16(Test_Data['Z'])\n",
    "normalization_factors = Test_Data['F']\n",
    "print(X_Test.shape)\n",
    "print(Y_Test.shape)\n",
    "print(house_labels.shape)\n",
    "print(normalization_factors.shape)\n",
    "house_labels_unique = np.unique(house_labels)\n",
    "num_houses = np.size(house_labels_unique)\n",
    "# Denormalize Waveforms for Evaluation\n",
    "Y_Test_shape = Y_Test.shape\n",
    "Y_Test_Denorm = np.zeros((Y_Test_shape[0],Y_Test_shape[1],Y_Test_shape[2]))\n",
    "for i in range(Y_Test_shape[0]):\n",
    "    Y_Test_Norm_Temp = Y_Test[i,:,:]\n",
    "    house_label_Temp = house_labels[0]\n",
    "    Y_Test_Denorm_Temp = Y_Test_Norm_Temp*normalization_factors[house_label_Temp*2-2]\n",
    "    Y_Test_Denorm[i,:,:] = Y_Test_Denorm_Temp\n",
    "# Load Resuts\n",
    "fl_Data = h5py.File(os.path.join('Outcomes/LDNet_Ch3_F1_P1_Kettle.h5'), 'r')\n",
    "print(fl_Data.keys())\n",
    "Y_Pred = fl_Data['App_Pred']\n",
    "Y_Pred_shape = Y_Pred.shape\n",
    "Y_Pred_Denorm = np.zeros((Y_Pred_shape[0],Y_Pred_shape[1],Y_Pred_shape[2]))\n",
    "for i in range(Y_Pred_shape[0]):\n",
    "    Y_Pred_Norm_Temp = Y_Pred[i,:,:]\n",
    "    house_label_Temp = house_labels[0]\n",
    "    Y_Pred_Denorm_Temp = Y_Pred_Norm_Temp*(normalization_factors[house_label_Temp*2-2])\n",
    "    Y_Pred_Denorm[i,:,:] = Y_Pred_Denorm_Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "data_shape = X_Test.shape\n",
    "i = random.randint(0, data_shape[0])\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(X_Test[i,:,0], label='GT')\n",
    "plt.ylim(0,1)\n",
    "plt.title(f\"Smart Meter (Power All Phase) -- Sample Number {i}\", fontdict={'fontsize': 16})\n",
    "plt.legend()\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(Y_Test_Denorm[i,:,0], label='Target')\n",
    "# plt.ylim(0,1)\n",
    "plt.title(f\"Appliance GT\", fontdict={'fontsize': 16})\n",
    "plt.legend()\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(Y_Pred_Denorm[i,:,0], label='Pred')\n",
    "# plt.ylim(0,1)\n",
    "plt.title(f\"Appliance Estimated\", fontdict={'fontsize': 16})\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Results\n"
     ]
    }
   ],
   "source": [
    "# Save Outcomes\n",
    "print('Saving Results')\n",
    "File = h5py.File(f'Outcomes/LDNet_5_32_DS_Kettle_SM3_21600_Overlapped.h5', 'w')\n",
    "File.create_dataset('SM', data=X_Test)\n",
    "File.create_dataset('App', data=Y_Test)\n",
    "File.create_dataset('App_Pred', data=Y_Pred)\n",
    "File.create_dataset('App_Denorm', data=Y_Test_Denorm)\n",
    "File.create_dataset('App_Pred_Denorm', data=Y_Pred_Denorm)\n",
    "File.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['App', 'App_Denorm', 'App_Pred', 'App_Pred_Denorm', 'SM']>\n"
     ]
    }
   ],
   "source": [
    "# Load Resuts\n",
    "fl_Data = h5py.File(os.path.join('Outcomes/Phase_1/LDNet/LDNet_5_32_DS_Dishwasher_SM3_21600_Overlapped.h5'), 'r')\n",
    "print(fl_Data.keys())\n",
    "X_Test = fl_Data['SM']\n",
    "Y_Test = fl_Data['App']\n",
    "Y_Pred = fl_Data['App_Pred']\n",
    "Y_Test_Denorm = fl_Data['App_Denorm']\n",
    "Y_Pred_Denorm = fl_Data['App_Pred_Denorm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Construction_Error(GRND, Pred):\n",
    "    mae_construction_err = []\n",
    "    mse_construction_err = []\n",
    "    rmse_construction_err = []\n",
    "    cc = []\n",
    "    for i in range(len(GRND)):\n",
    "        MAE = np.mean(np.abs(Pred[i].ravel() - GRND[i].ravel()))\n",
    "        MSE = mean_squared_error(Pred[i].ravel(), GRND[i].ravel())\n",
    "        RMSE = root_mean_squared_error(Pred[i].ravel(), GRND[i].ravel())\n",
    "        if ~(np.std(Pred[i].ravel()) == 0 or np.std(GRND[i].ravel()) == 0):\n",
    "            corr, _ = pearsonr(Pred[i].ravel(), GRND[i].ravel())\n",
    "        else:\n",
    "            continue\n",
    "        mae_construction_err.append(MAE)\n",
    "        mse_construction_err.append(MSE)\n",
    "        rmse_construction_err.append(RMSE)\n",
    "        cc.append(corr)\n",
    "    print(f'MAE Construction Error: {round(np.mean(mae_construction_err), 3)} +/- {round(np.std(mae_construction_err), 3)}')\n",
    "    print(f'MSE Construction Error: {round(np.mean(mse_construction_err), 3)} +/- {round(np.std(mse_construction_err), 3)}')\n",
    "    print(f'RMSE Construction Error: {round(np.mean(rmse_construction_err), 3)} +/- {round(np.std(rmse_construction_err), 3)}')\n",
    "    print(f'Pearson Correlation: {round(np.mean(cc)*100, 3)}% +/- {round(np.std(cc)*100, 3)}')\n",
    "    MAE = round(np.mean(mae_construction_err), 3)\n",
    "    MSE = round(np.mean(mse_construction_err), 3)\n",
    "    RMSE = round(np.mean(rmse_construction_err), 3)\n",
    "    PCC = round(np.mean(cc)*100, 3)\n",
    "    return MAE, MSE, RMSE, PCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Construction_Error(Y_Pred, Y_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculate_SAE(GRND, Pred):\n",
    "    EP = 0\n",
    "    EG = 0\n",
    "\n",
    "    for i in range(len(GRND)):\n",
    "        EG = EG + np.sum(GRND[i].ravel())\n",
    "        EP = EP + np.sum(Pred[i].ravel())\n",
    "        \n",
    "    SAE = np.abs(EP - EG)/EG\n",
    "    print(f'SAE Construction Error: {round(SAE, 3)}')\n",
    "    return SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Calculate_SAE(Y_Test_Denorm, Y_Pred_Denorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculate_EA(GRND, Pred):\n",
    "    EA = []\n",
    "    for i in range(len(GRND)):\n",
    "        GRND_Temp = GRND[i].ravel()\n",
    "        Pred_Temp = Pred[i].ravel()\n",
    "        NOM_Temp_TOT = 0\n",
    "        DENOM_Temp_TOT = 2*(np.sum(GRND_Temp))\n",
    "        for ii in range(len(GRND_Temp)):\n",
    "            NOM_Temp = np.abs(GRND_Temp[ii] - Pred_Temp[ii])\n",
    "            NOM_Temp_TOT = NOM_Temp_TOT + NOM_Temp\n",
    "        NOM_Temp = 1 - (NOM_Temp_TOT/DENOM_Temp_TOT)\n",
    "        EA.append(NOM_Temp)\n",
    "    EA_OVR = round(np.mean(EA), 3)\n",
    "    print(f'Estimation Accuracy (EA): {EA_OVR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Calculate_EA(Y_Test_Denorm, Y_Pred_Denorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JEOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculate_JEOI(GRND, Pred):\n",
    "    JEOI = []\n",
    "    for i in range(len(GRND)):\n",
    "        GRND_Temp = GRND[i].ravel()\n",
    "        Pred_Temp = Pred[i].ravel()\n",
    "        EO_Temp_TOT = 0\n",
    "        EE_Temp_TOT = 0\n",
    "        EM_Temp_TOT = 0\n",
    "        EP_Temp_TOT = np.sum(Pred_Temp)\n",
    "        EG_Temp_TOT = np.sum(GRND_Temp)\n",
    "        for ii in range(len(GRND_Temp)):\n",
    "            if Pred_Temp[ii] < 0:\n",
    "                Pred_Temp[ii] = 0\n",
    "            if GRND_Temp[ii] > Pred_Temp[ii]:\n",
    "                EO_Temp = Pred_Temp[ii]\n",
    "                EE_Temp = 0\n",
    "                EM_Temp = np.abs(GRND_Temp[ii] - Pred_Temp[ii])\n",
    "            elif GRND_Temp[ii] < Pred_Temp[ii]:\n",
    "                EO_Temp = GRND_Temp[ii]\n",
    "                EE_Temp = np.abs(Pred_Temp[ii] - GRND_Temp[ii])\n",
    "                EM_Temp = 0\n",
    "            elif GRND_Temp[ii] == Pred_Temp[ii]:\n",
    "                EO_Temp = GRND_Temp[ii]\n",
    "                EE_Temp = 0\n",
    "                EM_Temp = 0\n",
    "            # EO_Temp = min(GRND_Temp[ii], Pred_Temp[ii])\n",
    "            # EE_Temp = max(GRND_Temp[ii], Pred_Temp[ii])\n",
    "            EO_Temp_TOT = EO_Temp_TOT + EO_Temp\n",
    "            EE_Temp_TOT = EE_Temp_TOT + EE_Temp\n",
    "            EM_Temp_TOT = EM_Temp_TOT + EM_Temp\n",
    "            \n",
    "        EO_Temp_Norm = EO_Temp_TOT/EG_Temp_TOT\n",
    "        EE_Temp_Norm = EE_Temp_TOT/EG_Temp_TOT\n",
    "        EM_Temp_Norm = EM_Temp_TOT/EG_Temp_TOT\n",
    "        JEOI_Temp = EO_Temp_TOT/(EO_Temp_TOT+EE_Temp_TOT+EM_Temp_TOT)\n",
    "        JEOI.append(JEOI_Temp)\n",
    "    JEOI_OVR = round(np.mean(JEOI), 4)\n",
    "    print(f'JEOI: {JEOI_OVR}')\n",
    "    return JEOI_OVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Calculate_JEOI(Y_Test, Y_Pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculate_DEOI(GRND, Pred):\n",
    "    DEOI = []\n",
    "    for i in range(len(GRND)):\n",
    "        GRND_Temp = GRND[i].ravel()\n",
    "        Pred_Temp = Pred[i].ravel()\n",
    "        EO_Temp_TOT = 0\n",
    "        EE_Temp_TOT = 0\n",
    "        EM_Temp_TOT = 0\n",
    "        EP_Temp_TOT = np.sum(Pred_Temp)\n",
    "        EG_Temp_TOT = np.sum(GRND_Temp)\n",
    "        for ii in range(len(GRND_Temp)):\n",
    "            if Pred_Temp[ii] < 0:\n",
    "                Pred_Temp[ii] = 0\n",
    "            if GRND_Temp[ii] > Pred_Temp[ii]:\n",
    "                EO_Temp = Pred_Temp[ii]\n",
    "                EE_Temp = 0\n",
    "                EM_Temp = np.abs(GRND_Temp[ii] - Pred_Temp[ii])\n",
    "            elif GRND_Temp[ii] < Pred_Temp[ii]:\n",
    "                EO_Temp = GRND_Temp[ii]\n",
    "                EE_Temp = np.abs(Pred_Temp[ii] - GRND_Temp[ii])\n",
    "                EM_Temp = 0\n",
    "            elif GRND_Temp[ii] == Pred_Temp[ii]:\n",
    "                EO_Temp = GRND_Temp[ii]\n",
    "                EE_Temp = 0\n",
    "                EM_Temp = 0\n",
    "            # EO_Temp = min(GRND_Temp[ii], Pred_Temp[ii])\n",
    "            # EE_Temp = max(GRND_Temp[ii], Pred_Temp[ii])\n",
    "            EO_Temp_TOT = EO_Temp_TOT + EO_Temp\n",
    "            EE_Temp_TOT = EE_Temp_TOT + EE_Temp\n",
    "            EM_Temp_TOT = EM_Temp_TOT + EM_Temp\n",
    "        EO_Temp_Norm = EO_Temp_TOT/EG_Temp_TOT\n",
    "        EE_Temp_Norm = EE_Temp_TOT/EG_Temp_TOT\n",
    "        EM_Temp_Norm = EM_Temp_TOT/EG_Temp_TOT\n",
    "        DEOI_Temp = (2*EO_Temp_TOT)/((2*EO_Temp_TOT)+EE_Temp_TOT+EM_Temp_TOT)\n",
    "        DEOI.append(DEOI_Temp)\n",
    "    DEOI_OVR = round(np.mean(DEOI), 4)\n",
    "    print(f'DEOI: {DEOI_OVR}')\n",
    "    return DEOI_OVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Calculate_DEOI(Y_Test, Y_Pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WesN7p7AeYK4"
   },
   "source": [
    "# Infinite Loop to Keep the Tab Alive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0HqQ3O6XeZ1f"
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "tgW7r0C9TuZk",
    "Kir80l1FKPXB",
    "vMHdM26iekBm",
    "VUciLEJeyvyN",
    "WesN7p7AeYK4"
   ],
   "machine_shape": "hm",
   "name": "1D_CNN_Segmentation_End2End_Pipeline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
